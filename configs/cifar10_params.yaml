project: "cifar_grads"
name: "5k_baseline_attack"
dataset: "CIFAR10"

task: Cifar10
synthesizer: Pattern

batch_size: 128
test_batch_size: 100
lr: 0.001
momentum: 0.9
decay: 0.0005
epochs: 50
save_on_epochs: []
optimizer: Adam
log_interval: 100

#opacus: True
#saved_grads: True
#max_batch_id: 100
grad_sigma: 0.05
grad_clip: 1
#batch_clip: True

#resume_model: /home/eugene/backdoors/saved_models/model_Cifar10_baseline/model_last.pt.tar
#recover_indices: weights/weights_5k_001.pt
#cosine_batching: True
#compute_grads_from_resumed_model: /home/eugene/backdoors/saved_models/model_Cifar10_baseline/model_last.pt.tar
clamp_norms: 0.1
pow_weight: 2
#cut_grad_threshold: 7.5

pretrained: True

scheduler: True
scheduler_milestones: [150, 225]

poisoning_proportion: 0.001
backdoor_label: 8
backdoor: True

drop_label_proportion: 0.97
drop_label: 5

loss_balance: none
mgda_normalize: loss+

#pre_compute_grads: True
#cosine_batching: True
#sampling_model_epochs: 10
#gradient_layer: 'fc.weight'
#de_sample: 0.005
  #'conv1.weight'
  #'fc.weight'

#save_model: True
log: True
wandb: True
tb: False
plot_conf_matrix: True

transform_train: True

subset_training:
  part: 5000

loss_tasks:
  - normal
