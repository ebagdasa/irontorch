project: "cifar_grads"
name: "ds01_cb7_cp_10"
dataset: "CIFAR10"

task: Cifar10
synthesizer: Pattern

batch_size: 128
test_batch_size: 100
lr: 0.001
momentum: 0.9
decay: 0.0005
epochs: 50
save_on_epochs: []
optimizer: Adam
log_interval: 100

#opacus: True
#saved_grads: True
#max_batch_id: 100
grad_sigma: 0.05
grad_clip: 1
#batch_clip: True

#resume_model: /home/eugene/backdoors/saved_models/model_Cifar10_baseline/model_last.pt.tar
#recover_indices: weights/weights_5k_001.pt
#cosine_batching: True
#compute_grads_from_resumed_model: /home/eugene/backdoors/saved_models/model_Cifar10_baseline/model_last.pt.tar
clamp_norms: 0.1
pow_weight: 2
#cut_grad_threshold: 7.5

pretrained: True

scheduler: True
scheduler_milestones: [150, 225]

poisoning_proportion: 0.001
backdoor_label: 8
backdoor: True

drop_label_proportion: 0.9
drop_label: 5

loss_balance: none
mgda_normalize: loss+

pre_compute_grads: True
cosine_batching: True
sampling_model_epochs: 10
gradient_layer: 'fc.weight'
de_sample: 0.01
cosine_bound: 0.7
clamp_probs: 10.0
#add_images_to_clean: True

  #'conv1.weight'
  #'fc.weight'

#save_model: True
log: True
wandb: True
tb: False
plot_conf_matrix: True

transform_train: True

subset_training:
  part: 5000

loss_tasks:
  - normal

poison_images:
  - 389
  - 561
  - 874
  - 1605
  - 3378
  - 3678
  - 4528
  - 9744
  - 19165
  - 19500
  - 21422
  - 22984
  - 32941
  - 34287
  - 34385
  - 36005
  - 37365
  - 37533
  - 38658
  - 38735
  - 39824
  - 40138
  - 41336
  - 41861
  - 47001
  - 47026
  - 48003
  - 48030
  - 49163
  - 49588