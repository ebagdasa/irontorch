project: "cifar_grads_full_0005"
name: "bc1_s002_ds1_cb7_cp1_cn1"
dataset: "CIFAR10"

task: Cifar10
synthesizer: Pattern

batch_size: 128
test_batch_size: 100
lr: 0.1
momentum: 0.9
decay: 0.0005
epochs: 200
save_on_epochs: []
optimizer: SGD
log_interval: 100

#opacus: True
batch_clip: True
#saved_grads: True
grad_sigma: 0.002
grad_clip: 1

#resume_model: /home/eugene/backdoors/saved_models/model_Cifar10_baseline/model_last.pt.tar
#compute_grads_from_resumed_model: /home/eugene/backdoors/saved_models/model_Cifar10_baseline/model_last.pt.tar

#pow_weight: 2

pretrained: False

scheduler: True
scheduler_milestones: [150, 225]
#
poisoning_proportion: 0.0005
backdoor_label: 8
backdoor: True
##
drop_label_proportion: 0.9
drop_label: 5
#
#cut_grad_threshold: 12
#
clean_subset: 5000
#
pre_compute_grads: True
sampling_model_epochs: 50
gradient_layer: 'linear.weight'  # 'fc.weight'
cosine_batching: True
de_sample: 0.1
cosine_bound: 0.7
clamp_probs: 1.0
clamp_norms: 1.0

#add_images_to_clean: True
  #'conv1.weight'
  #'fc.weight'

#save_model: True
#tb: False
log: True
wandb: True
plot_conf_matrix: True

transform_train: True



loss_balance: none
mgda_normalize: loss+
loss_tasks:
  - normal

poison_images:
  - 389
  - 561
  - 874
  - 1605
  - 3378
  - 3678
  - 4528
  - 9744
  - 19165
  - 19500
  - 21422
  - 22984
  - 32941
  - 34287
  - 34385
  - 36005
  - 37365
  - 37533
  - 38658
  - 38735
  - 39824
  - 40138
  - 41336
  - 41861
  - 47001
  - 47026
  - 48003
  - 48030
  - 49163
  - 49588