project: "cifar_grads"
name: "grads_01_5k"
dataset: "CIFAR10"

task: Cifar10
synthesizer: Pattern

batch_size: 128
test_batch_size: 100
lr: 0.1
momentum: 0.9
decay: 0.0005
epochs: 200
save_on_epochs: []
optimizer: SGD
log_interval: 100

#opacus: True
#saved_grads: True
#max_batch_id: 100
grad_sigma: 0.05
grad_clip: 1
#batch_clip: True

#resume_model: model_Cifar10_cifar_init_5k/model_last.pt.tar
#recover_indices: weights/weights_5k_001.pt
#cosine_batching: True
clamp_norms: 0.1
pow_weight: 2
#cut_grad_threshold: 7.5

#pretrained: True

scheduler: True
scheduler_milestones: [150, 225]

poisoning_proportion: 0.01
backdoor_label: 8
backdoor: True

loss_balance: none
mgda_normalize: loss+

#fix_opacus_model: True
pre_compute_grads: True
#cosine_batching: True
sampling_model_epochs: 20

#save_model: True
#tb: False
#log: True
#wandb: True

transform_train: True

subset_training:
  part: 10000

loss_tasks:
#  - backdoor
  - normal
#  - neural_cleanse
#  - sentinet_evasion



#poison_images_test:
#  - 389
#  - 561
#  - 874
#  - 1605
#  - 3378
#  - 3678
#  - 4528
#  - 9744
#  - 19165
#  - 19500
#  - 21422
#  - 22984
#  - 32941
#  - 34287
#  - 34385
#  - 36005
#  - 37365
#  - 37533
#  - 38658
#  - 38735
#  - 39824
#  - 40138
#  - 41336
#  - 41861
#  - 47001
#  - 47026
#  - 48003
#  - 48030
#  - 49163
#  - 49588
#
#poison_images:
#  - 389
#  - 561
#  - 874
#  - 1605
#  - 3378
#  - 3678
#  - 4528
#  - 9744
#  - 19165
#  - 19500
#  - 21422
#  - 22984
#  - 32941
#  - 34287
#  - 34385
#  - 36005
#  - 37365
#  - 37533
#  - 38658
#  - 38735
#  - 39824
#  - 40138
#  - 41336
#  - 41861
#  - 47001
#  - 47026
#  - 48003
#  - 48030
#  - 49163
#  - 49588